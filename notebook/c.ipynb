{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Optional\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    self.features = [(user_id, [context_item_id])]\n",
    "    self.target_item = [target_item_id]\n",
    "    self.clv = [clv_value]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequences: dict[int, list[int]],\n",
    "        clv_dict: dict[int, float],\n",
    "        target_items: dict[int, list[int]],\n",
    "        context_length: int = 10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences (dict[int, list[int]]):\n",
    "                ユーザごとの購買商品系列\n",
    "                [user_id : user_items]\n",
    "            clv_dict (dict[int, float]):\n",
    "                ユーザのCLV\n",
    "                [user_id : clv_value]\n",
    "            context_length (int, optional):\n",
    "                予測時に参照する直前のアイテムの個数\n",
    "        \"\"\"\n",
    "        self.features = []\n",
    "        self.target_items = []\n",
    "        self.clv = []\n",
    "\n",
    "        for user_idx, sequence in tqdm(sequences.items()):\n",
    "            self.features.append((user_idx, sequence))\n",
    "            self.target_items.append(target_items[user_idx] if user_idx in target_items else [])\n",
    "            self.clv.append(clv_dict[user_idx] if user_idx in clv_dict else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[tuple[int, list[int]], list[int], float]:\n",
    "        return self.features[idx], self.target_items[idx], self.clv[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_features(feature_df: pd.DataFrame, split_date: pd.Timestamp):\n",
    "    # 最新購買日\n",
    "    recency = split_date - feature_df.groupby(\"user_id\").timestamp.max()\n",
    "    recency = (recency.dt.days / 365).rename(\"recency\")\n",
    "    recency[recency >= 1] = 1\n",
    "\n",
    "    # 総購買数\n",
    "    frequency = feature_df.user_id.value_counts().sort_index().rename(\"frequency\")\n",
    "    frequency /= 500\n",
    "    frequency[frequency >= 1] = 1\n",
    "\n",
    "    # 利用期間\n",
    "    tenure = (\n",
    "        feature_df.groupby(\"user_id\").timestamp.max()\n",
    "        - feature_df.groupby(\"user_id\").timestamp.min()\n",
    "    )\n",
    "    tenure = (tenure.dt.days / 365).rename(\"tenure\")\n",
    "    tenure[tenure >= 1] = 1\n",
    "\n",
    "    user_features = pd.merge(recency, tenure, on=\"user_id\", how=\"left\")\n",
    "    user_features = pd.merge(user_features, frequency, on=\"user_id\", how=\"left\")\n",
    "    return user_features\n",
    "\n",
    "\n",
    "def create_targets(target_df: pd.DataFrame):\n",
    "    # CLV\n",
    "    clv_dict = target_df.user_id.value_counts().sort_index().to_dict()\n",
    "\n",
    "    # CV商品\n",
    "    target_items = target_df.groupby(\"user_id\").item_id.agg(list).to_dict()\n",
    "    return clv_dict, target_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_user_item_id(\n",
    "    interaction_df: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "    user_le = LabelEncoder().fit(interaction_df.user_id)\n",
    "    item_le = LabelEncoder().fit(interaction_df.item_id)\n",
    "\n",
    "    interaction_df.user_id = user_le.transform(interaction_df.user_id)\n",
    "    interaction_df.item_id = item_le.transform(interaction_df.item_id)\n",
    "\n",
    "    return interaction_df, user_le, item_le\n",
    "\n",
    "\n",
    "def load_word2vec(word2vec_model_path: Optional[str]) -> Optional[gensim.models.word2vec.Word2Vec]:\n",
    "    if word2vec_model_path is not None:\n",
    "        try:\n",
    "            model = gensim.models.word2vec.Word2Vec.load(word2vec_model_path)\n",
    "            return model\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    interaction_df: pd.DataFrame,\n",
    "    split_date: pd.Timestamp | str,\n",
    "    context_length: int = 10,\n",
    "    d_model: int = 32,\n",
    "    window: int = 5,\n",
    "    word2vec_model_path: Optional[str] = None,\n",
    ") -> tuple[Dataset, torch.Tensor, torch.Tensor]:\n",
    "    feature_df = (\n",
    "        interaction_df[interaction_df.timestamp < split_date]\n",
    "        .sort_values(\"timestamp\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    target_df = (\n",
    "        interaction_df[interaction_df.timestamp >= split_date]\n",
    "        .sort_values(\"timestamp\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    sequences = feature_df.groupby(\"user_id\").item_id.agg(list).to_dict()\n",
    "    user_features = create_user_features(feature_df=feature_df, split_date=split_date)\n",
    "    user_embedding_weight = torch.from_numpy(user_features.values.astype(np.float32))\n",
    "\n",
    "    clv_dict, target_items = create_targets(target_df=target_df)\n",
    "\n",
    "    model = load_word2vec(word2vec_model_path)\n",
    "    if model is None:\n",
    "        model = gensim.models.word2vec.Word2Vec(\n",
    "            sentences=sequences.values(),\n",
    "            vector_size=d_model,\n",
    "            window=window,\n",
    "            min_count=1,\n",
    "        )\n",
    "        if word2vec_model_path is not None:\n",
    "            model.save(word2vec_model_path)\n",
    "    item_embedding_weight = torch.from_numpy(model.wv.vectors.astype(np.float32))\n",
    "\n",
    "    dataset = Dataset(\n",
    "        sequences=sequences,\n",
    "        clv_dict=clv_dict,\n",
    "        target_items=target_items,\n",
    "        context_length=context_length,\n",
    "    )\n",
    "    return dataset, user_embedding_weight, item_embedding_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_embedding_weight: torch.Tensor,\n",
    "        item_embedding_weight: torch.Tensor,\n",
    "        d_model: int = 64,\n",
    "        nhead: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        item_size, _ = item_embedding_weight.shape\n",
    "        user_size, user_feature_dim = user_embedding_weight.shape\n",
    "\n",
    "        self.user_feature = torch.nn.Embedding(user_size, user_feature_dim)\n",
    "        self.user_feature.weight = torch.nn.Parameter(user_embedding_weight)\n",
    "        self.user_feature.weight.require_grad = False\n",
    "\n",
    "        self.user_embedding = torch.nn.Linear(user_feature_dim, d_model)\n",
    "\n",
    "        self.item_embedding = torch.nn.Embedding(item_size, d_model)\n",
    "        self.item_embedding.weight = torch.nn.Parameter(item_embedding_weight)\n",
    "        self.item_embedding.weight.require_grad = False\n",
    "\n",
    "        self.transformer_layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id: torch.Tensor, item_indices: torch.Tensor):\n",
    "        e_u = self.user_feature.forward(user_id)\n",
    "        e_u = self.user_embedding.forward(e_u)\n",
    "        H_v = self.item_embedding.forward(item_indices)\n",
    "        return e_u, H_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2006-05-17 15:34:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2006-05-17 12:26:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2006-05-17 12:27:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2006-05-17 15:13:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2006-05-17 12:21:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000090</th>\n",
       "      <td>162541</td>\n",
       "      <td>50872</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-04-28 21:16:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000091</th>\n",
       "      <td>162541</td>\n",
       "      <td>55768</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2009-04-28 20:53:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000092</th>\n",
       "      <td>162541</td>\n",
       "      <td>56176</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2009-04-28 20:31:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000093</th>\n",
       "      <td>162541</td>\n",
       "      <td>58559</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2009-04-28 21:17:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000094</th>\n",
       "      <td>162541</td>\n",
       "      <td>63876</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009-04-28 21:01:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000095 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  item_id  rating           timestamp\n",
       "0               1      296     5.0 2006-05-17 15:34:04\n",
       "1               1      306     3.5 2006-05-17 12:26:57\n",
       "2               1      307     5.0 2006-05-17 12:27:08\n",
       "3               1      665     5.0 2006-05-17 15:13:40\n",
       "4               1      899     3.5 2006-05-17 12:21:50\n",
       "...           ...      ...     ...                 ...\n",
       "25000090   162541    50872     4.5 2009-04-28 21:16:12\n",
       "25000091   162541    55768     2.5 2009-04-28 20:53:18\n",
       "25000092   162541    56176     2.0 2009-04-28 20:31:37\n",
       "25000093   162541    58559     4.0 2009-04-28 21:17:14\n",
       "25000094   162541    63876     5.0 2009-04-28 21:01:55\n",
       "\n",
       "[25000095 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_df = pd.read_csv(\"../data/ml-25m/ratings.csv\")\n",
    "interaction_df.timestamp = pd.to_datetime(interaction_df.timestamp, unit=\"s\")\n",
    "interaction_df = interaction_df.rename(columns={\"userId\": \"user_id\", \"movieId\": \"item_id\"})\n",
    "interaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_review_date = pd.to_datetime(\"2015/01/01\")\n",
    "train_split_date = pd.to_datetime(\"2017/01/01\")\n",
    "\n",
    "first_review = interaction_df.groupby(\"user_id\").timestamp.min()\n",
    "last_review = interaction_df.groupby(\"user_id\").timestamp.max()\n",
    "target_users = set(last_review[(first_review < train_split_date)].index) & set(\n",
    "    last_review[(last_review_date <= last_review)].index\n",
    ")\n",
    "\n",
    "train_interaction_df = interaction_df[interaction_df.user_id.isin(target_users)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25093/25093 [00:00<00:00, 230833.12it/s]\n"
     ]
    }
   ],
   "source": [
    "context_length = 10\n",
    "d_model = 32\n",
    "window = 5\n",
    "batch_size = 16\n",
    "\n",
    "train_interaction_df, user_le, item_le = encode_user_item_id(\n",
    "    interaction_df=train_interaction_df\n",
    ")\n",
    "\n",
    "# FIXME: 2018年以降の予測対象に含んでいる\n",
    "(\n",
    "    train_dataset,\n",
    "    train_user_embedding_weight,\n",
    "    train_item_embedding_weight,\n",
    ") = create_dataset(\n",
    "    interaction_df=train_interaction_df,\n",
    "    split_date=train_split_date,\n",
    "    context_length=context_length,\n",
    "    d_model=d_model,\n",
    "    window=window,\n",
    "    word2vec_model_path=\"./word2vec.model\",\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb#X55sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m user_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(user_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb#X55sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m context_items \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(context_items)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb#X55sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39;49mforward(user_ids, context_items)\n",
      "\u001b[1;32m/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb#X55sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m e_u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_feature\u001b[39m.\u001b[39mforward(user_id)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb#X55sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m e_u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_embedding\u001b[39m.\u001b[39mforward(e_u)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb#X55sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m H_v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitem_embedding\u001b[39m.\u001b[39;49mforward(item_indices)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatsuyaishii/dev/my-master-thesis/notebook/c.ipynb#X55sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m e_u, H_v\n",
      "File \u001b[0;32m~/dev/my-master-thesis/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/dev/my-master-thesis/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    user_embedding_weight=train_user_embedding_weight,\n",
    "    item_embedding_weight=train_item_embedding_weight,\n",
    "    d_model=d_model,\n",
    "    nhead=8,\n",
    ")\n",
    "\n",
    "for a in train_dataloader:\n",
    "    user_ids = []\n",
    "    context_items = []\n",
    "    for (user_id, user_context_items), target_items, clv in a:\n",
    "        user_ids.append(user_id)\n",
    "        context_items.append(user_context_items[:1])\n",
    "\n",
    "    user_ids = torch.LongTensor(user_ids)\n",
    "    context_items = torch.LongTensor(context_items)\n",
    "    model.forward(user_ids, context_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
